# 聚类：无监督学习。

## 性能度量：
簇内相似度高且簇间相似度低。

## 距离计算：
闵可夫斯基距离：欧氏距离(p=2) or 曼哈顿距离(p=1)

## 1）划分聚类  
对于给定的数据集，划分聚类需要知道要划分簇的数目k（k<=n, n是数据集中项的数目）。划分聚类将数据分为k组，每组至少有一项。大多数划分聚类都是基于距离的。 一般情况下给出了聚类数目k，首先会产生一个初始的划分。然后用迭代的方法通过更改数据项所属的簇来提高划分的质量。一个好的划分的标准是同一个簇内的数据项彼此相似，相反地，不同簇的项有较大的区别。实现全局最优划分往往很难在复杂度忍受的范围内做到。然而，大多数应用都选取了一些启发式方法。比如像选取贪心策略的k-means和k-medoids算法，都极大地提高了划分质量，并达到了一个局部最优解。这些启发式聚类算法在中小型数据集中挖掘类似球形簇表现非常好。

（2）层次聚类  层次聚类就是通过对数据集按照某种方法进行层次分解，直到满足某种条件为止。层次聚类根据划分的方法分为凝聚和分割的两种。凝聚的方法也叫做自底向上方法。它每次迭代将最相近两个项（或者组）合并形成一个新的组。直至最终形成一个组或者达到其他停止的条件。分割的方法也叫自顶向下，与凝聚的方法相反。开始的时候讲所有数据看成一个组，每一次迭代一个簇就被划分成两个小一点簇。直到最终每个项都是一个簇或者达到了某个停止条件。层次聚类可以是基于距离、基于密度、基于连接的。层次聚类有一个缺点：一旦一个凝聚或分割形成了，这个操作永远不能再更改了。这样的好处就是可以计算复杂度相对较小。

（3）基于密度的聚类 很多聚类算法都是根据距离计算的。 这样子的话很容易发现球形的簇，然后很难发现其他形状的簇。基于密度的算法认为，在整个样本空间点中，各目标类簇是由一群的稠密样本点组成的，而这些稠密样本点被低密度区域（噪声）分割，而算法的目的就是要过滤低密度区域，发现稠密样本点。这类算法往往重视数据项的密集程度，因此这些算法都是基于连接的。虽然是基于连接的，但是也强调了连接过程中数据项周围的密度。这样就能发现各种任意形状的聚类簇。

（4）基于网格的聚类  这类算法将数据项的空间划分成有限数目的网格。所有的聚类操作都是在网格上进行的。这样最大的好处是可以计算速度相当快。因为计算过程跟数据项的数目没有关系，只与每一维网格的数目和维数有关系。对于大数据的数据挖掘问题，网格的方法效率往往会很不错。然而我觉得网格只是一种思想，这种思想往往要和其他的算法相结合才能解决好实际问题，比如聚类。

## 1. 原型聚类

1）k均值聚类（k-means clustering）：最小化平方误差。

从样本集中随机选择k个样本作为初始均值向量——计算每个样本与各均值向量的距离，将样本划入距离最近的均值向量的簇内——求出新的均值向量，并更新——不断重复上述过程知道没有变化为止。

2）学习向量量化（learning vector quantization，LVQ）

LVQ假设数据样本带有类别标记，学习过程利用样本的这些监督信息来辅助聚类。

初始化一组原型向量——找出与有标记的训练样本最近的原型向量，并根据两者的类别标记是否一致来对原型向量进行相应的更新（若类别不同，则原型向量与样本之间的距离增大）——迭代更新直到原型向量不再更新或者更新很小

3）高斯混合聚类（mixture-of-gaussian）：采用概率模型来表达聚类原型。

## 2. 密度聚类：假设聚类结构能通过样本分布的紧密程度确定

DBSCAN：先找出各样本的邻域并确定核心对象集合Ω，然后从Ω中随机选取一个核心对象作为种子，找出由它密度可达的所有样本，这就构成了第一个聚类簇。

3. 层次聚类：在不同层次对数据集进行划分，从而形成树形的聚类结构。

AGNES：是一种采用自底向上聚合策略的层次聚类算法。它先将数据集中的每个样本看作一个初始聚类簇，然后在算法运行的每一步中找出距离
